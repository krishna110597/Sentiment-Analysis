{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishna110597/Sentiment-Analysis/blob/main/Tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import html.entities as htmlentitydefs"
      ],
      "metadata": {
        "id": "F6n3r_pYcbPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoticon_string = r\"\"\"\n",
        "    (?:\n",
        "      [<>]?\n",
        "      [:;=8]                     # eyes\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
        "      |\n",
        "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
        "      [\\-o\\*\\']?                 # optional nose\n",
        "      [:;=8]                     # eyes\n",
        "      [<>]?\n",
        "    )\"\"\"\n",
        "regex_strings = (\n",
        "    # Phone numbers:\n",
        "    r\"\"\"\n",
        "    (?:\n",
        "      (?:            # (international)\n",
        "        \\+?[01]\n",
        "        [\\-\\s.]*\n",
        "      )?            \n",
        "      (?:            # (area code)\n",
        "        [\\(]?\n",
        "        \\d{3}\n",
        "        [\\-\\s.\\)]*\n",
        "      )?    \n",
        "      \\d{3}          # exchange\n",
        "      [\\-\\s.]*   \n",
        "      \\d{4}          # base\n",
        "    )\"\"\"\n",
        "    ,\n",
        "    # Emoticons:\n",
        "    emoticon_string\n",
        "    ,\n",
        "    # url\n",
        "    r\"\"\"(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+\"\"\",    \n",
        "    # HTML tags:\n",
        "    r\"\"\"<[^>]+>\"\"\"\n",
        "    ,\n",
        "    # Twitter username:\n",
        "    r\"\"\"(?:@[\\w_]+)\"\"\"\n",
        "    ,\n",
        "    # Twitter hashtags:\n",
        "    r\"\"\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\"\"\n",
        "    ,\n",
        "    # Remaining word types:\n",
        "    r\"\"\"\n",
        "    (?:[a-z][a-z'\\-_]+[a-z])       # Words with apostrophes or dashes.\n",
        "    |\n",
        "    (?:[+\\-]?\\d+[,/.:-]\\d+[+\\-]?)  # Numbers, including fractions, decimals.\n",
        "    |\n",
        "    (?:[\\w_]+)                     # Words without apostrophes or dashes.\n",
        "    |\n",
        "    (?:\\.(?:\\s*\\.){1,})            # Ellipsis dots. \n",
        "    |\n",
        "    (?:\\S)                         # Everything else that isn't whitespace.\n",
        "    \"\"\"\n",
        "    )"
      ],
      "metadata": {
        "id": "nOukoYm2c-ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the core tokenizing regex:\n",
        "    \n",
        "word_re = re.compile(r\"\"\"(%s)\"\"\" % \"|\".join(regex_strings), re.VERBOSE | re.I | re.UNICODE)\n",
        "\n",
        "# The emoticon string gets its own regex so that we can preserve case for them as needed:\n",
        "emoticon_re = re.compile(regex_strings[1], re.VERBOSE | re.I | re.UNICODE)\n",
        "\n",
        "# These are for regularizing HTML entities to Unicode:\n",
        "html_entity_digit_re = re.compile(r\"&#\\d+;\")\n",
        "html_entity_alpha_re = re.compile(r\"&\\w+;\")\n",
        "amp = \"&amp;\"   "
      ],
      "metadata": {
        "id": "PxOLIDXudQyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, preserve_case=False):\n",
        "        self.preserve_case = preserve_case\n",
        "\n",
        "    def tokenize(self, s):\n",
        "        \"\"\"\n",
        "        Argument: s -- any string or unicode object\n",
        "        Value: a tokenize list of strings; conatenating this list returns the original string if preserve_case=False\n",
        "        \"\"\"        \n",
        "        # Try to ensure unicode:\n",
        "        try:\n",
        "            s = str(s)\n",
        "        except UnicodeDecodeError:\n",
        "            s = bytes(s).encode('string_escape')\n",
        "            s = str(s)\n",
        "        # Fix HTML character entitites:\n",
        "        s = self.__html2unicode(s)\n",
        "        # Tokenize:\n",
        "        words = word_re.findall(s)\n",
        "        # Possible alter the case, but avoid changing emoticons like :D into :d:\n",
        "        if not self.preserve_case:            \n",
        "            words = map((lambda x : x if emoticon_re.search(x) else x.lower()), words)\n",
        "        return words\n",
        "\n",
        "    def tokenize_random_tweet(self):\n",
        "        \"\"\"\n",
        "        If the twitter library is installed and a twitter connection\n",
        "        can be established, then tokenize a random tweet.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import twitter\n",
        "        except ImportError:\n",
        "            print (\"Apologies. The random tweet functionality requires the Python twitter library: http://code.google.com/p/python-twitter/\")\n",
        "        from random import shuffle\n",
        "        api = twitter.Api()\n",
        "        tweets = api.GetPublicTimeline()\n",
        "        if tweets:\n",
        "            for tweet in tweets:\n",
        "                if tweet.user.lang == 'en':            \n",
        "                    return self.tokenize(tweet.text)\n",
        "        else:\n",
        "            raise Exception(\"Apologies. I couldn't get Twitter to give me a public English-language tweet. Perhaps try again\")\n",
        "\n",
        "    def __html2unicode(self, s):\n",
        "        \"\"\"\n",
        "        Internal metod that seeks to replace all the HTML entities in\n",
        "        s with their corresponding unicode characters.\n",
        "        \"\"\"\n",
        "        # First the digits:\n",
        "        ents = set(html_entity_digit_re.findall(s))\n",
        "        if len(ents) > 0:\n",
        "            for ent in ents:\n",
        "                entnum = ent[2:-1]\n",
        "                try:\n",
        "                    entnum = int(entnum)\n",
        "                    s = s.replace(ent, chr(entnum))\t\n",
        "                except:\n",
        "                    pass\n",
        "        s = s.replace(amp, \" and \")\n",
        "        # Now the alpha versions:\n",
        "        ents = set(html_entity_alpha_re.findall(s))\n",
        "        #ents = filter((lambda x : x != amp), ents)\n",
        "        for ent in ents:\n",
        "            entname = ent[1:-1]\n",
        "            try:            \n",
        "                s = s.replace(ent, chr(htmlentitydefs.name2codepoint[entname]))\n",
        "            except:\n",
        "                pass                    \n",
        "        return s\n"
      ],
      "metadata": {
        "id": "wrTirEm2duUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    tok = Tokenizer(preserve_case=False)\n",
        "    samples = (\n",
        "        u\"Extremely disturbing reports from UttarPradesh. The spread in rural areas is much higher in #COVIDSecondWave. With #PanchayatElections starting from tomorrow - people are afraid of rapid spread due to political activity. They should consider postponing the polls. #COVID19\",\n",
        "        u\"As Delhi reported the highest single-day spike with 17,282 new #COVID19 cases and 104 deaths, amidst a shortage of beds, hotels and banquets set aside beds for Covid patients.https://t.co/1jZQAasQ0v\")\n",
        "\n",
        "    for s in samples:\n",
        "        print (\"======================================================================\")\n",
        "        print (s)\n",
        "        tokenized = tok.tokenize(s)\n",
        "        print (\"\\n\".join(tokenized))\n"
      ],
      "metadata": {
        "id": "DP1uLV49eW6W",
        "outputId": "66f60ea6-9ebf-4d35-b529-918a884d1c57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Extremely disturbing reports from UttarPradesh. The spread in rural areas is much higher in #COVIDSecondWave. With #PanchayatElections starting from tomorrow - people are afraid of rapid spread due to political activity. They should consider postponing the polls. #COVID19\n",
            "extremely\n",
            "disturbing\n",
            "reports\n",
            "from\n",
            "uttarpradesh\n",
            ".\n",
            "the\n",
            "spread\n",
            "in\n",
            "rural\n",
            "areas\n",
            "is\n",
            "much\n",
            "higher\n",
            "in\n",
            "#covidsecondwave\n",
            ".\n",
            "with\n",
            "#panchayatelections\n",
            "starting\n",
            "from\n",
            "tomorrow\n",
            "-\n",
            "people\n",
            "are\n",
            "afraid\n",
            "of\n",
            "rapid\n",
            "spread\n",
            "due\n",
            "to\n",
            "political\n",
            "activity\n",
            ".\n",
            "they\n",
            "should\n",
            "consider\n",
            "postponing\n",
            "the\n",
            "polls\n",
            ".\n",
            "#covid19\n",
            "======================================================================\n",
            "As Delhi reported the highest single-day spike with 17,282 new #COVID19 cases and 104 deaths, amidst a shortage of beds, hotels and banquets set aside beds for Covid patients.https://t.co/1jZQAasQ0v\n",
            "as\n",
            "delhi\n",
            "reported\n",
            "the\n",
            "highest\n",
            "single-day\n",
            "spike\n",
            "with\n",
            "17,282\n",
            "new\n",
            "#covid19\n",
            "cases\n",
            "and\n",
            "104\n",
            "deaths\n",
            ",\n",
            "amidst\n",
            "a\n",
            "shortage\n",
            "of\n",
            "beds\n",
            ",\n",
            "hotels\n",
            "and\n",
            "banquets\n",
            "set\n",
            "aside\n",
            "beds\n",
            "for\n",
            "covid\n",
            "patients.https\n",
            ":/\n",
            "/t.co/1jzqaasq0v\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}